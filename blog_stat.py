import os
import frontmatter
from datetime import date
from dateutil import parser # å¯¼å…¥æ—¥æœŸè§£æåº“
from collections import Counter # æ–°å¢å¯¼å…¥ï¼šç”¨äºé«˜æ•ˆç»Ÿè®¡æ ‡ç­¾é¢‘ç‡

# 1. é…ç½®å‚æ•°
# ----------------------------------------------------
# å‡è®¾æ‚¨çš„æ–‡ç« éƒ½åœ¨è¿™ä¸ªç›®å½•ä¸‹
# ä¿®å¤äº†è·¯å¾„é—®é¢˜ï¼Œä½¿ç”¨æ­£æ–œæ æˆ–åŸå§‹å­—ç¬¦ä¸²
CONTENT_DIR = "D:/Github Repo/Blog/content/posts"
# å¹´åº¦æŠ¥å‘Šå¹´ä»½
TARGET_YEAR = 2025
# è¾“å‡ºçš„ Markdown æ–‡ä»¶å
OUTPUT_FILE = f"Annual_Report_{TARGET_YEAR}_Blog_Stats.md"
# ----------------------------------------------------

def count_words(filepath):
    """
    ä¸€ä¸ªç®€å•çš„å­—æ•°ç»Ÿè®¡å‡½æ•°ï¼ˆå¿½ç•¥ Front Matterï¼‰
    FIXED: é’ˆå¯¹ä¸­æ–‡å†…å®¹ï¼Œæ”¹ä¸ºç»Ÿè®¡éç©ºç™½å­—ç¬¦æ•°
    """
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            # æ‰¾åˆ° Front Matter çš„ç»“æŸä½ç½® (ç¬¬äºŒä¸ª '---')
            parts = content.split('---', 2)
            if len(parts) < 3:
                 # å¦‚æœæ²¡æœ‰ Front Matterï¼Œç»Ÿè®¡å…¨éƒ¨å†…å®¹
                 body = content
            else:
                 # æå–æ–‡ç« ä¸»ä½“ (å¿½ç•¥ Front Matter)
                 body = parts[2]
            
            # --- ä¿®å¤ç‚¹ï¼šé’ˆå¯¹ä¸­æ–‡ç»Ÿè®¡å­—æ•° ---
            # body.split() ä¼šæŒ‰æ‰€æœ‰ç©ºç™½ç¬¦ï¼ˆç©ºæ ¼ã€æ¢è¡Œç­‰ï¼‰åˆ†å‰²ï¼Œå¾—åˆ°éç©ºç™½å†…å®¹çš„åˆ—è¡¨
            # "".join() å°†è¿™äº›å†…å®¹é‡æ–°è¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸å«ä»»ä½•ç©ºç™½ç¬¦çš„çº¯æ–‡æœ¬
            # len() è®¡ç®—çš„å°±æ˜¯çº¯æ–‡æœ¬çš„å­—ç¬¦æ•°ï¼Œè¿™å¯¹äºä¸­æ–‡ç»Ÿè®¡æ˜¯å‡†ç¡®çš„ã€‚
            character_count = len("".join(body.split()))
            return character_count
            
    except Exception as e:
        # ç»Ÿä¸€å¤„ç†æ–‡ä»¶è¯»å–å’Œå­—æ•°ç»Ÿè®¡ä¸­çš„é”™è¯¯
        print(f"Error counting words in {filepath}: {e}")
        return 0

def generate_stats_table():
    """
    éå†æ–‡ç« ç›®å½•ï¼Œæå–æ•°æ®ã€èšåˆç»Ÿè®¡å¹¶ç”Ÿæˆ Markdown è¡¨æ ¼
    """
    all_posts_data = []
    # æ–°å¢ï¼šç”¨äºèšåˆç»Ÿè®¡çš„å˜é‡
    tag_frequency = Counter() # æ ‡ç­¾é¢‘ç‡è®¡æ•°å™¨
    total_word_count = 0     # æ€»å­—æ•°è®¡æ•°å™¨
    
    # éå†ç›®å½•
    for root, _, files in os.walk(CONTENT_DIR):
        for filename in files:
            if filename.endswith((".md", ".markdown")):
                filepath = os.path.join(root, filename)
                
                try:
                    # ä½¿ç”¨ frontmatter åº“è§£ææ–‡ä»¶
                    with open(filepath, 'r', encoding='utf-8') as f:
                        post = frontmatter.load(f)
                        metadata = post.metadata
                        
                        post_date_raw = metadata.get('date')
                        post_date = None
                        
                        # === å¥å£®çš„æ—¥æœŸè§£æï¼ˆä¸ºåç»­å¹´ä»½æ£€æŸ¥åšå‡†å¤‡ï¼‰ ===
                        if post_date_raw:
                            try:
                                if isinstance(post_date_raw, str):
                                    post_date = parser.parse(post_date_raw)
                                else:
                                    post_date = post_date_raw
                                    
                            except Exception:
                                print(f"Warning: Cannot parse date for file {filename} (Raw: {post_date_raw}). Skipping year check.")
                        
                        # === æ ¸å¿ƒè¿‡æ»¤è§„åˆ™ï¼šæ’é™¤è‰ç¨¿ã€éšè—æ–‡ç« ã€æ—¥æœŸç¼ºå¤±/ä¸ç¬¦åˆè¦æ±‚çš„æ–‡ç«  ===
                        
                        # 1. æ’é™¤è‰ç¨¿ (draft: true)
                        if metadata.get('draft') is True:
                            print(f"Skipping file {filename}: Draft is set to True.")
                            continue
                            
                        # 2. æ’é™¤éšè—æ–‡ç«  (hidden: true)
                        if metadata.get('hidden') is True:
                            print(f"Skipping file {filename}: Hidden is set to True.")
                            continue

                        # 3. æ’é™¤æ—¥æœŸç¼ºå¤±çš„æ–‡ç« 
                        if not post_date_raw:
                             print(f"Skipping file {filename}: Date is missing.")
                             continue
                             
                        # 4. æ’é™¤æ—¥æœŸä¸åœ¨ç›®æ ‡å¹´ä»½çš„æ–‡ç« 
                        if not (post_date and hasattr(post_date, 'year') and post_date.year == TARGET_YEAR):
                             print(f"Skipping file {filename}: Date ({post_date}) is not in {TARGET_YEAR}")
                             continue
                            
                        # === æå–æ‰€éœ€æ•°æ®ï¼ˆå·²é€šè¿‡æ‰€æœ‰æ£€æŸ¥ï¼‰ ===
                        title = metadata.get('title', 'N/A')
                        description = str(metadata.get('description', 'N/A')) 
                        
                        # æ ‡ç­¾å®‰å…¨å¤„ç†ï¼šç¡®ä¿æ˜¯åˆ—è¡¨å¹¶è¿‡æ»¤ None
                        tags_list = metadata.get('tags', [])
                        if not isinstance(tags_list, list):
                            tags_list = [] 
                        
                        clean_tags_list = [str(t) for t in tags_list if t is not None]
                            
                        tags = ", ".join(clean_tags_list) if clean_tags_list else 'N/A'
                        word_count = count_words(filepath)
                        
                        # === èšåˆç»Ÿè®¡ï¼šæ›´æ–°æ€»å­—æ•°å’Œæ ‡ç­¾é¢‘ç‡ ===
                        total_word_count += word_count
                        tag_frequency.update(clean_tags_list)
                        
                        # å°†æ•°æ®æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                        all_posts_data.append({
                            'Title': title,
                            'Description': description,
                            'Tags': tags,
                            'Date': post_date.strftime('%Y-%m-%d'), # æ ¼å¼åŒ–æ—¥æœŸ
                            'Word_Count': word_count
                        })
                            
                except Exception as e:
                    # æ•è· Front Matter è§£æè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å…¶ä»–é”™è¯¯
                    print(f"Skipping file {filepath} due to critical parsing error: {e}")

    # 2. æ’åº (å¯é€‰: æŒ‰æ—¥æœŸæ’åº)
    all_posts_data.sort(key=lambda x: x['Date'])

    # 3. ç”Ÿæˆ Markdown è¾“å‡ºå†…å®¹
    
    final_markdown_output = []
    num_articles = len(all_posts_data)
    
    # ===============================================
    # 3.1 ç”Ÿæˆæ•°æ®æ¦‚è§ˆéƒ¨åˆ†ï¼ˆæ ‡ç­¾é¢‘ç‡å’Œæ€»å­—æ•°ï¼‰
    # ===============================================
    
    avg_word_count = int(total_word_count / num_articles) if num_articles > 0 else 0

    final_markdown_output.extend([
        f"## ğŸ“Š {TARGET_YEAR} å¹´åº¦åšå®¢æ•°æ®æ¦‚è§ˆ",
        "",
        "| æŒ‡æ ‡ | ç»Ÿè®¡ç»“æœ |",
        "| :--- | :---: |",
        f"| **æ–‡ç« æ€»æ•°** | {num_articles} ç¯‡ |",
        f"| **æ€»å­—æ•°** | {total_word_count:,} å­— |", # æ ¼å¼åŒ–æ•°å­—ï¼Œå¸¦åƒä½åˆ†éš”ç¬¦
        f"| **å¹³å‡å­—æ•°** | {avg_word_count:,} å­— |",
        "",
        "### ğŸ·ï¸ æ ‡ç­¾é¢‘ç‡ç»Ÿè®¡ (æŒ‰ä½¿ç”¨æ¬¡æ•°é™åº)",
    ])
    
    # æ ¼å¼åŒ–æ ‡ç­¾é¢‘ç‡åˆ—è¡¨
    tag_list_markdown = []
    if tag_frequency:
        for tag, count in tag_frequency.most_common():
            tag_list_markdown.append(f"- **{tag}**: {count} ç¯‡")
    else:
        tag_list_markdown.append("- æœ¬å¹´åº¦æ–‡ç« ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆæ ‡ç­¾ã€‚")

    final_markdown_output.extend(tag_list_markdown)
    final_markdown_output.append("\n") # ç¡®ä¿æ¦‚è§ˆå’Œåˆ—è¡¨ä¹‹é—´æœ‰ç©ºè¡Œ
    
    # ===============================================
    # 3.2 ç”Ÿæˆæ–‡ç« åˆ—è¡¨éƒ¨åˆ†
    # ===============================================

    final_markdown_output.extend([
        f"## ğŸ“… {TARGET_YEAR} å¹´åº¦åšå®¢æ–‡ç« åˆ—è¡¨ ({num_articles} ç¯‡)",
        "",
        "| åºå· | å‘å¸ƒæ—¥æœŸ | æ–‡ç« æ ‡é¢˜ | æ ‡ç­¾ | æ‘˜è¦/æè¿° | å­—æ•° |",
        "| :---: | :---: | :--- | :--- | :--- | :---: |"
    ])
    
    # è¡¨æ ¼è¡Œ
    for i, data in enumerate(all_posts_data, 1):
        # é™åˆ¶ Description é•¿åº¦ï¼Œé¿å…è¡¨æ ¼è¿‡å®½
        short_desc = (data['Description'][:50].replace('\n', ' ') + '...') if len(data['Description']) > 50 else data['Description'].replace('\n', ' ')
        
        row = (
            f"| {i} "
            f"| {data['Date']} "
            f"| **{data['Title']}** "
            f"| {data['Tags']} "
            f"| {short_desc} "
            f"| {data['Word_Count']} |"
        )
        final_markdown_output.append(row)

    # 4. å†™å…¥æ–‡ä»¶
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write('\n'.join(final_markdown_output))
        
    print(f"\nâœ… ç»Ÿè®¡å®Œæˆï¼Markdown è¡¨æ ¼å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")
    print(f"å…±ç»Ÿè®¡åˆ° {num_articles} ç¯‡ {TARGET_YEAR} å¹´çš„æ–‡ç« ã€‚")


if __name__ == "__main__":
    if not os.path.isdir(CONTENT_DIR):
        print(f"âŒ é”™è¯¯: ç›®å½• {CONTENT_DIR} ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥ CONTENT_DIR å˜é‡è®¾ç½®ã€‚")
    else:
        generate_stats_table()